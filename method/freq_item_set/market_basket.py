# -*- coding: utf-8 -*-
"""market basket

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wxgo5gWMeadMBOIhca278d-1l9T2j-Yg

# 0 Init
"""

import csv
import os
import json
import argparse
import random
import re
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter



from tqdm import tqdm

# # spelling
# from spellchecker import SpellChecker

# nltk
import nltk
from nltk.corpus import stopwords

# from nltk.stem import PorterStemmer
# from nltk.tokenize import word_tokenize

# # language detector
# from langdetect import detect
# from langdetect import DetectorFactory
# DetectorFactory.seed = 0

from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

tqdm.pandas()
nltk.download('stopwords')
nltk.download('punkt')

all_profile = pd.read_csv("../../data/processed_data.csv")

"""# 2 Market Basket for Attributes"""

attribute_col = ['username','scam','age_bin','gender','location','ethnicity','marital_status','children','religion','sexual_orientation']
basket_col = ['scam','age_bin','gender','location','ethnicity','marital_status','children','religion','sexual_orientation']

all_profile_attribute = all_profile[attribute_col]
all_profile_attribute.dropna(subset=['username'], inplace=True)
all_profile_attribute.set_index('username', inplace=True)

# scam and non-scam
all_profile_attribute_scam = all_profile_attribute[all_profile_attribute['scam'] == 1]
all_profile_attribute_nonscam = all_profile_attribute[all_profile_attribute['scam'] == 0]

# change column value into "col name - value" format
def transform_row(row):
    return [f"{column} - {value}" for column, value in row.items() if pd.notna(value)]

all_profile_attribute_scam_b = all_profile_attribute_scam[basket_col].apply(transform_row, axis=1)
all_profile_attribute_nonscam_b = all_profile_attribute_nonscam[basket_col].apply(transform_row, axis=1)

def onehot_enc(df):
  encoder = TransactionEncoder().fit(df)
  onehot = encoder.transform(df)
  df = pd.DataFrame(onehot, columns = encoder.columns_)
  return df

all_profile_attribute_scam_enc = onehot_enc(all_profile_attribute_scam_b)
all_profile_attribute_nonscam_enc = onehot_enc(all_profile_attribute_nonscam_b)

# Run Apriori algorithm to find frequent itemsets
frequent_itemsets = apriori(all_profile_attribute_scam_enc, min_support=0.2, use_colnames=True)

# Generate association rules
scam_rules_raw = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
scam_rules_raw = scam_rules_raw[scam_rules_raw['consequents'] == {'scam - 1'}]

# Run Apriori algorithm to find frequent itemsets
frequent_itemsets = apriori(all_profile_attribute_nonscam_enc, min_support=0.2, use_colnames=True)

# Generate association rules
nonscam_rules_raw = association_rules(frequent_itemsets, metric="lift", min_threshold=1)
nonscam_rules_raw = nonscam_rules_raw[nonscam_rules_raw['consequents'] == {'scam - 0'}]

# remove duplicates
scam_antecedents_set = set(scam_rules_raw['antecedents'])
nonscam_antecedents_set = set(nonscam_rules_raw['antecedents'])

scam_antecedents_set_processed = scam_antecedents_set - nonscam_antecedents_set
nonscam_antecedents_set_processed = nonscam_antecedents_set - scam_antecedents_set

scam_rules = scam_rules_raw[scam_rules_raw['antecedents'].isin(scam_antecedents_set_processed)]
nonscam_rules = nonscam_rules_raw[nonscam_rules_raw['antecedents'].isin(nonscam_antecedents_set_processed)]

scam_rules_top7 = scam_rules[['antecedents', 'consequents', 'support']].sort_values(by='support', ascending=False)

nonscam_rules_top7 = nonscam_rules[['antecedents', 'consequents', 'support']].sort_values(by='support', ascending=False)[:7]

scam_rules_top7 = [set(x) for x in scam_rules_top7['antecedents']]
nonscam_rules_top7 = [set(x) for x in nonscam_rules_top7['antecedents']]

"""# 4 Validation"""

seed_value = 99999
np.random.seed(seed_value)
sample_size = 500

samples_scam = pd.DataFrame(all_profile_attribute_scam_b.sample(n=sample_size, random_state=seed_value))
samples_nonscam = pd.DataFrame(all_profile_attribute_nonscam_b.sample(n=sample_size, random_state=seed_value))

def calculate_marks(samples_df):
    # Initialize a new column for marks
    samples_df['marks'] = 0

    # Iterate over each sample
    for index, row in samples_df.iterrows():
        sample = set(row[0])

        # Iterate over each scam rule, if sample set contains rule set, then -5 score
        for scam_rule in scam_rules_top7:
            l = len(scam_rule)
            if scam_rule <= sample:
                samples_df.at[index, 'marks'] -= 0.5*l

        # Iterate over each nonscam rule, if sample set contains rule set, then +5 score
        for nonscam_rule in nonscam_rules_top7:
            l = len(nonscam_rule)
            if nonscam_rule <= sample:
                samples_df.at[index, 'marks'] += 0.5*l

    return samples_df

samples_scam_scored = calculate_marks(samples_scam)
samples_nonscam_scored = calculate_marks(samples_nonscam)

score_distribution_scam = samples_scam['marks']
score_distribution_nonscam = samples_nonscam['marks']

# Create subplots
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

# Plot the score distribution for scam samples
axes[0].hist(score_distribution_scam, bins=20, edgecolor='black')
axes[0].set_title('Scam Samples')
axes[0].set_xlabel('Score')
axes[0].set_ylabel('Frequency')

# Plot the score distribution for non-scam samples
axes[1].hist(score_distribution_nonscam, bins=20, edgecolor='black')
axes[1].set_title('Non-Scam Samples')
axes[1].set_xlabel('Score')
axes[1].set_ylabel('Frequency')

# Adjust layout to prevent clipping of titles
plt.tight_layout()

# Show the plot
plt.show()

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming samples_scam and samples_nonscam are your DataFrames with the 'marks' column

# Add 'actual_class' column based on whether the sample is from samples_scam
samples_scam['actual_class'] = 'scam'
samples_nonscam['actual_class'] = 'non-scam'

# Combine scam and non-scam sets
combined_samples = pd.concat([samples_scam, samples_nonscam], ignore_index=True)

# Filter out samples with a score of 0
combined_samples_filtered = combined_samples[combined_samples['marks'] != 0]

# Predicted class based on marks
combined_samples_filtered['predicted_class'] = combined_samples_filtered['marks'].apply(lambda x: 'scam' if x < 0 else 'non-scam')

# Create a confusion matrix
conf_matrix = confusion_matrix(combined_samples_filtered['actual_class'], combined_samples_filtered['predicted_class'], labels=['scam', 'non-scam'])

# Plot the confusion matrix using seaborn
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['scam', 'non-scam'], yticklabels=['scam', 'non-scam'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Class')
plt.ylabel('True Class')
plt.show()

# Calculate percentage of samples with score != 0 for each dataset
percentage_not_zero_combined = (combined_samples_filtered.shape[0] / combined_samples.shape[0]) * 100
percentage_not_zero_scam = (samples_scam[samples_scam['marks'] != 0].shape[0] / samples_scam.shape[0]) * 100
percentage_not_zero_nonscam = (samples_nonscam[samples_nonscam['marks'] != 0].shape[0] / samples_nonscam.shape[0]) * 100

print(f"Percentage of samples with non-zero score in combined samples: {percentage_not_zero_combined:.2f}%")
print(f"Percentage of samples with non-zero score in scam samples: {percentage_not_zero_scam:.2f}%")
print(f"Percentage of samples with non-zero score in non-scam samples: {percentage_not_zero_nonscam:.2f}%")

# Calculate percentage of correct predictions for samples with score != 0
correct_predictions_combined = np.diag(conf_matrix).sum() / conf_matrix.sum() * 100
correct_predictions_scam = conf_matrix[0, 0] / conf_matrix[0, :].sum() * 100
correct_predictions_nonscam = conf_matrix[1, 1] / conf_matrix[1, :].sum() * 100

print(f"Percentage of correct predictions in combined samples with non-zero score: {correct_predictions_combined:.2f}%")
print(f"Percentage of correct predictions in scam samples with non-zero score: {correct_predictions_scam:.2f}%")
print(f"Percentage of correct predictions in non-scam samples with non-zero score: {correct_predictions_nonscam:.2f}%")

combined_samples = pd.concat([samples_scam, samples_nonscam], ignore_index=False)
combined_samples.to_csv('fis_samples.csv')
